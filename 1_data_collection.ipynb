{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook description\n",
    "\n",
    "#### What is this notebook for? \n",
    "This this the first notebook of a serie of five. Its main purpose is to collect the data that will be used for the rest of the project. \n",
    "\n",
    "\n",
    "The process of data collection will be divided in multiple processes: \n",
    "1. Create a list of pages/account where I found bots' comments \n",
    "2. For each, I look at their last 50+ posts to collect the ID of the post \n",
    "3. Once done, loop through those thousands of IDs to collect the comments\n",
    "4. \n",
    "\n",
    "\n",
    "\n",
    "collected the IDs of  from multiple pages suseptible to have bots commenting on their posts. I selected those pages manually by making sure they're all bots' targets. Then, I'll loop through each post thanks to it's ID. On instagram, each post url is made as `instagram.com/p/{postid}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules and UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Personnal module, functions I use often\n",
    "import src.webscraping as mw\n",
    "import src.useful as mu\n",
    "\n",
    "\n",
    "# Better print \n",
    "from tqdm import tqdm\n",
    "\n",
    "# To become Dr Strange \n",
    "import time \n",
    "from datetime import datetime\n",
    "\n",
    "# Basic data \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Store data \n",
    "import sqlite3\n",
    "import json\n",
    "from flatten_json import flatten\n",
    "\n",
    "# Move things around locally\n",
    "import shutil\n",
    "\n",
    "# Fetch instagram data \n",
    "from instaloader import Instaloader, Profile\n",
    "from instaloader.exceptions import ProfileNotExistsException\n",
    "import urllib\n",
    "from splinter import Browser\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting notebook preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating SQL database to store all the data for this project\n",
    "database = \"data/main_database.sqlite\"\n",
    "con = sqlite3.connect(database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Collect post ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List a number of instagram accounts that have bots commenting on their posts. Here I'm looping through a page list that is targeted by bots and collect the posts_ids one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# During the first collection time I had 5 more\n",
    "pages = [\"nfl\", \"championsleague\", \"mercedesamgf1\", \"ESPN\", \"bleacherreport\", \"houseofhighlights\", \"nba\", \"worldstar\", \"grmdaily\", \"pubity\", \n",
    "         \"meme.ig\", \"brgridiron\", \"lakers\", \"ballislife\", \"nflonfox\", \"nflnetwork\", \"espnnfl\", \"cbssports\", \"thecheckdown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_ids = pd.DataFrame(columns=[\"post_id\", \"page\"])\n",
    "post_per_page = 50\n",
    "\n",
    "# Lunch browser\n",
    "browser = Browser('chrome')\n",
    "\n",
    "for page in pages:\n",
    "    browser.visit(f\"https://www.instagram.com/{page}/\")\n",
    "    postids = []\n",
    "    for i in list(range(10))[::-1]:\n",
    "        print(i, end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    while len(postids) < post_per_page:\n",
    "        # Scroll up and then down each time helps the page to not bug\n",
    "        browser.execute_script(\"window.scrollTo(0, 0);\")\n",
    "        time.sleep(0.5)\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Change the browser to a beautiful soup object where I can get the posts id\n",
    "        soup = mw.bsoup(browser)\n",
    "\n",
    "        # Instagram doesn't show in the html the posts it of the post we don't see so I need to slowly scroll down to collect each of them.\n",
    "        browser.execute_script(\"window.scrollBy(0, 200);\")\n",
    "\n",
    "        for element in soup.find_all(\"a\"):\n",
    "            link = element.get(\"href\")\n",
    "            \n",
    "            # We can find the posts id by looking into a tags that have an attribute of href.\n",
    "            if \"/p/\" in link:\n",
    "                postids.append(link.replace(\"/p/\", \"\").replace(\"/\", \"\"))\n",
    "\n",
    "        # Removing duplicates\n",
    "        postids = list(set(postids))\n",
    "\n",
    "        time.sleep(np.random.randint(30)/10)\n",
    "\n",
    "\n",
    "        post_page_id = [[post_id, page] for post_id in postids]\n",
    "\n",
    "        # Create a df with post_ids and save it in the db\n",
    "        df_new_post_ids = pd.DataFrame(post_page_id, columns=[\"post_id\", \"page_id\"])\n",
    "        df_new_post_ids.to_sql(\"post_ids\", con, if_exists=\"append\", index=False)\n",
    "        time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scrape comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>page_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CY23Dx-hQzz</td>\n",
       "      <td>br_hoops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CYhycdhh__L</td>\n",
       "      <td>br_hoops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CXpjIIagKAx</td>\n",
       "      <td>br_hoops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CYxi2doBjWu</td>\n",
       "      <td>br_hoops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CYDVbgjhJEq</td>\n",
       "      <td>br_hoops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7826</th>\n",
       "      <td>CWIYygLtmsJ</td>\n",
       "      <td>mercedesamgf1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7827</th>\n",
       "      <td>CZhKJh_uJne</td>\n",
       "      <td>mercedesamgf1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7828</th>\n",
       "      <td>CWROwfUMZR4</td>\n",
       "      <td>mercedesamgf1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7829</th>\n",
       "      <td>CYWw8V4tRPD</td>\n",
       "      <td>mercedesamgf1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7830</th>\n",
       "      <td>CYi8IhRNilj</td>\n",
       "      <td>mercedesamgf1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7831 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          post_id      page_name\n",
       "0     CY23Dx-hQzz       br_hoops\n",
       "1     CYhycdhh__L       br_hoops\n",
       "2     CXpjIIagKAx       br_hoops\n",
       "3     CYxi2doBjWu       br_hoops\n",
       "4     CYDVbgjhJEq       br_hoops\n",
       "...           ...            ...\n",
       "7826  CWIYygLtmsJ  mercedesamgf1\n",
       "7827  CZhKJh_uJne  mercedesamgf1\n",
       "7828  CWROwfUMZR4  mercedesamgf1\n",
       "7829  CYWw8V4tRPD  mercedesamgf1\n",
       "7830  CYi8IhRNilj  mercedesamgf1\n",
       "\n",
       "[7831 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query post_id not in comments table \n",
    "query = \"\"\"\n",
    "SELECT  \n",
    "    DISTINCT\n",
    "    post_id\n",
    "    , page_name\n",
    "FROM post_ids \n",
    "\"\"\"\n",
    "\n",
    "# Loadind post_ids from db\n",
    "posts_ids_to_scrape = pd.read_sql_query(query, con)\n",
    "posts_ids_to_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping and storing comments from post_ids:\n",
    "np.random.shuffle(posts_ids_to_scrape)\n",
    "for post_id in tqdm(posts_ids_to_scrape):\n",
    "    browser.visit(f\"https://www.instagram.com/p/{post_id}/\")\n",
    "    \n",
    "    while True: \n",
    "        # Get page soup \n",
    "        soup = mw.bsoup(browser)\n",
    "        if soup.find(\"h2\", class_=\"_a9zc\") != None: \n",
    "            break\n",
    "\n",
    "    # Get days since posted\n",
    "    post_posted_time = soup.find(\"time\", class_=\"_a9ze _a9zf\").get(\"datetime\")\n",
    "    now = datetime.now()\n",
    "    days_diff = (now - pd.to_datetime(post_posted_time[:-1])).days\n",
    "\n",
    "    # I only keep what was posted less than a month ago\n",
    "    if days_diff > 31: \n",
    "        continue\n",
    "    \n",
    "    df_post_comments = pd.DataFrame(columns=[\"post_id\", \"page\", \"legend\", \"post_posted_time\", \"username\", \n",
    "                                                       \"full_comment_data\", \"comment\", \"comment_posted_time\", \n",
    "                                                       \"time_since_posted\", \"comments_likes\", \"replies\",\n",
    "                                                       \"time_now\"])\n",
    "    for comment_block in soup.find_all(\"ul\", class_=\"_a9ym\"):\n",
    "        page = soup.find(\"h2\", class_=\"_a9zc\").text \n",
    "        legend = soup.find(\"div\", class_=\"_a9zs\").text \n",
    "        time_since_posted = comment_block.find(\"time\", class_=\"_a9ze _a9zf\").text \n",
    "        username = comment_block.find(\"h3\", class_=\"_a9zc\").text \n",
    "        comment_posted_time = comment_block.find(\"time\", class_=\"_a9ze _a9zf\").get(\"datetime\")\n",
    "        comments_likes = comment_block.find(\"button\", class_=\"_a9ze\").text \n",
    "        comment = comment_block.find(\"div\", class_=\"_a9zs\").text  \n",
    "        replies = comment_block.find(\"li\", class_=\"_a9yg\").text if comment_block.find(\"li\", class_=\"_a9yg\") != None else \"\"\n",
    "        full_comment_data = comment_block.text\n",
    "        time_now = datetime.now()\n",
    "\n",
    "        # Add comment values to dataframe\n",
    "        df_post_comments.loc[len(df_post_comments)] = [post_id, page, legend, post_posted_time, username, \n",
    "                                                       full_comment_data, comment, comment_posted_time, \n",
    "                                                       time_since_posted, comments_likes, replies,\n",
    "                                                       time_now]\n",
    "\n",
    "        df_post_comments[\"period\"] = \"summer\"\n",
    "\n",
    "        \n",
    "    df_post_comments.to_sql(\"comments\", con, if_exists=\"append\", index=False)\n",
    "    time.sleep(np.random.randint(40, 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Collect user data from comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate Instaloader\n",
    "L = Instaloader()\n",
    "\n",
    "def fetch_user_data (username):\n",
    "    '''Function to fetch an Instagram user's public data.\n",
    "\n",
    "    Parameter: \n",
    "        username str: username of the user to collect the \n",
    "        data from.\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        profile = Profile.from_username(L.context, username)\n",
    "    except ProfileNotExistsException:\n",
    "        return f\"{username} does not exists anymore\"\n",
    "        \n",
    "    data = profile.__dict__\n",
    "    del data[\"_context\"]\n",
    "    json_object = json.dumps(data, indent = 2)   \n",
    "    with open(f\"data/users_json/{username}_user_profile_data.json\", 'w') as file_object:  \n",
    "        json.dump(json_object, file_object) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json (username):\n",
    "    # Open json \n",
    "    try:\n",
    "        json_file = open(f\"data/users_json/{username}_user_profile_data.json\")\n",
    "        data = json.loads(json.load(json_file))[\"_node\"]\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        return\n",
    "\n",
    "\n",
    "    # useles json keys\n",
    "    ban = ['edge_felix_video_timeline', 'edge_owner_to_timeline_media', 'edge_saved_media', \n",
    "            'edge_media_collections', 'edge_related_profiles']\n",
    "\n",
    "    # Defining basic keys. Those are name, follower count, bio, etc. Basic infos\n",
    "    basic_keys = [key for key in data.keys() if key not in ban]\n",
    "\n",
    "    basic_info = flatten({key:data[key] for key in basic_keys})\n",
    "    df_current_user = pd.DataFrame([basic_info])\n",
    "    # username = df_current_user.loc[0, \"username\"]\n",
    "\n",
    "    # Getting the data for all posts that are contained in lists. \n",
    "    df_current_user[\"video_count\"] = data['edge_felix_video_timeline'][\"count\"]\n",
    "    df_current_user[\"post_count\"] = data['edge_owner_to_timeline_media'][\"count\"]\n",
    "\n",
    "    last_12_posts = dict()\n",
    "    posts = data['edge_owner_to_timeline_media'][\"edges\"]\n",
    "    last_12_posts[\"username\"] = data[\"username\"]\n",
    "    last_12_posts[\"video_views\"] = [post[\"node\"][\"video_view_count\"] if \"video_view_count\" in post[\"node\"].keys() else np.nan for post in posts]\n",
    "    last_12_posts[\"display_url\"] = [post[\"node\"][\"display_url\"] for post in posts]\n",
    "    last_12_posts[\"thumbnail_src\"] = [post[\"node\"][\"thumbnail_src\"] for post in posts]\n",
    "    last_12_posts[\"accessibility_caption\"] = [post[\"node\"][\"accessibility_caption\"] for post in posts]\n",
    "    last_12_posts[\"is_video\"] = [post[\"node\"][\"is_video\"] for post in posts]\n",
    "    last_12_posts[\"likes\"] = [post[\"node\"][\"edge_liked_by\"][\"count\"] for post in posts]\n",
    "    last_12_posts[\"comments\"] = [post[\"node\"][\"edge_media_to_comment\"][\"count\"] for post in posts]\n",
    "    last_12_posts[\"timestamp\"] = [post[\"node\"][\"taken_at_timestamp\"] for post in posts]\n",
    "    df_last_12_posts = pd.DataFrame(last_12_posts)\n",
    "\n",
    "\n",
    "    # Changing list type to str as sqlite3 doesn't accept this type\n",
    "    list_features = ['bio_links', 'biography_with_entities_entities', 'edge_mutual_followed_by_edges', 'pronouns']\n",
    "    for column in list_features: \n",
    "        if column in df_current_user.columns:\n",
    "            df_current_user[column] = df_current_user[column].apply(lambda x:  \"_LIST_SEPARATOR_\".join(x))\n",
    "\n",
    "    return df_current_user, df_last_12_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:06<00:00,  2.08s/it]\n"
     ]
    }
   ],
   "source": [
    "all_folders = ['/Volumes/Lacie /igbotdata/website_screenshot',\n",
    "'/Users/marclamy/Desktop/code/all ig bot folders/igbot_final/data/website_screenshot',\n",
    "'/Users/marclamy/Desktop/code/all ig bot folders/instagram_bot_detection/data/url_screenshot']\n",
    "\n",
    "all_files_full_path = [mu.list_files(folder) for folder in tqdm(all_folders)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17537"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files_full_path = [file for lst in all_files_full_path for file in lst]\n",
    "dict_all_files_full_path = {fp.split('/')[-1]: fp for fp in all_files_full_path}\n",
    "\n",
    "\n",
    "clean_dict_all_files_full_path = {}\n",
    "\n",
    "for key, value in dict_all_files_full_path.items():\n",
    "    if key not in clean_dict_all_files_full_path.keys():\n",
    "        clean_dict_all_files_full_path[key] = value\n",
    "\n",
    "len(clean_dict_all_files_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "449762"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done_users_photos = mu.list_files('data/photos/user_profile_pictures', common=False)\n",
    "done_12_photos = mu.list_files('data/photos/user_last_12_posts', common=False)\n",
    "\n",
    "len(done_users_photos) + len(done_12_photos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17537"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = list(clean_dict_all_files_full_path.keys())\n",
    "np.random.shuffle(keys)\n",
    "clean_dict_all_files_full_path = {key: clean_dict_all_files_full_path[key] for key in keys}\n",
    "len(clean_dict_all_files_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_dict_all_files_full_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/marclamy/Desktop/code/Instagram_bot_classification/1_dead_data_collection.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/marclamy/Desktop/code/Instagram_bot_classification/1_dead_data_collection.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# done_users_photos = mu.list_files('data/photos/user_profile_pictures')\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/marclamy/Desktop/code/Instagram_bot_classification/1_dead_data_collection.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# done_users_photos = [file.split('/')[-1] for file in done_users_photos]\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/marclamy/Desktop/code/Instagram_bot_classification/1_dead_data_collection.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# done_12_photos = mu.list_files('data/photos/user_last_12_posts')\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/marclamy/Desktop/code/Instagram_bot_classification/1_dead_data_collection.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# done_12_photos = [file.split('/')[-1] for file in done_12_photos]\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/marclamy/Desktop/code/Instagram_bot_classification/1_dead_data_collection.ipynb#X31sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m all_df \u001b[39m=\u001b[39m []\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/marclamy/Desktop/code/Instagram_bot_classification/1_dead_data_collection.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m file_name, current_path \u001b[39min\u001b[39;00m tqdm(\u001b[39mlist\u001b[39m(clean_dict_all_files_full_path\u001b[39m.\u001b[39mitems())):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/marclamy/Desktop/code/Instagram_bot_classification/1_dead_data_collection.ipynb#X31sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     df_photos \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/marclamy/Desktop/code/Instagram_bot_classification/1_dead_data_collection.ipynb#X31sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     new_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata/photos/\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "# done_users_photos = mu.list_files('data/photos/user_profile_pictures')\n",
    "# done_users_photos = [file.split('/')[-1] for file in done_users_photos]\n",
    "# done_12_photos = mu.list_files('data/photos/user_last_12_posts')\n",
    "# done_12_photos = [file.split('/')[-1] for file in done_12_photos]\n",
    "\n",
    "all_df = []\n",
    "for file_name, current_path in tqdm(list(clean_dict_all_files_full_path.items())):\n",
    "    df_photos = pd.DataFrame()\n",
    "    new_path = 'data/photos/'\n",
    "    # print(f'{file_name}')\n",
    "\n",
    "\n",
    "    if file_name not in done_users_photos and file_name not in done_12_photos:\n",
    "        # if '_pp_user_photo' in file_name:\n",
    "        username = file_name.replace('_website_photo.png', '')\n",
    "        new_path = new_path + 'bio_url_screenshot/' + file_name\n",
    "        shutil.copy(current_path, new_path)\n",
    "        df_photos.loc[len(df_photos), ['username', 'photo_type', 'path']] = [username, 'bio_url', new_path]\n",
    "\n",
    "        # else:\n",
    "        #     photo_index = file_name.split('_')[-3]\n",
    "        #     username = file_name.replace(f'_{photo_index}_user_photo.png', '')\n",
    "        #     new_path = new_path + 'user_last_12_posts/' + file_name\n",
    "        #     shutil.copy(current_path, new_path)\n",
    "        #     df_photos.loc[len(df_photos), ['username', 'photo_type', 'path']] = [username, f'{photo_index}', new_path]\n",
    "\n",
    "    all_df.append(df_photos)\n",
    "\n",
    "df = pd.concat(all_df, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17537"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_sql('photos', con, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query post_id not in comments table \n",
    "query = \"\"\"\n",
    "SELECT  \n",
    "    DISTINCT\n",
    "    username\n",
    "FROM comments \n",
    "WHERE 1=1\n",
    "    --AND username NOT IN (SELECT username FROM users)\n",
    "    --AND username NOT IN (SELECT username FROM errors where error_type='user_not_found')\"\"\"\n",
    "\n",
    "# Loadind usernames to scrape from db\n",
    "usernames_to_scrape = pd.read_sql_query(query, con)[\"username\"]\n",
    "print(f\"Usernames to scrape: {len(usernames_to_scrape)}\")\n",
    "\n",
    "\n",
    "for index, username in enumerate(tqdm(usernames_to_scrape[:5])): \n",
    "    # print(username, username, username, username)\n",
    "    fetch_user_data(username)\n",
    "    \n",
    "    try:\n",
    "        df_current_user, df_current_last_12_posts = convert_json(username)\n",
    "    except FileNotFoundError: # If an error happenned while fetching the data, no file\n",
    "        continue\n",
    "\n",
    "    # Not all users have the same number of columns returned and SQL needs same cols so got to use this way\n",
    "    if index != 0: # No table exists before first index\n",
    "        df_users = pd.read_sql_query('select * from users', con)\n",
    "    df_users = pd.concat([df_users, df_current_user]).drop_duplicates()\n",
    "    df_users.to_sql(\"users\", con, if_exists=\"replace\", index=False)\n",
    "    df_current_last_12_posts.to_sql(\"last_12_posts\", con, if_exists=\"append\", index=False)\n",
    "\n",
    "\n",
    "    # Profile pic\n",
    "    for username, profile_pic_url in df_current_user[[\"username\",  \"profile_pic_url\"]].values: \n",
    "        try:\n",
    "            urllib.request.urlretrieve(profile_pic_url, f\"data/photos/test/{username}_pp_user_photo.png\")\n",
    "        except Exception as e:\n",
    "            print(username, e, end='\\r')\n",
    "\n",
    "    # Last 12 posts\n",
    "    df_current_last_12_posts = df_current_last_12_posts.reset_index()\n",
    "    for username, display_url, index in df_current_last_12_posts[[\"username\", \"display_url\", \"index\"]].values: \n",
    "        urllib.request.urlretrieve(display_url, f\"data/photos/test/{username}_{index}_user_photo.png\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Screenshot bio url landing page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>external_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mr.145th</td>\n",
       "      <td>https://onlyfans.com/letmeholdthatsoul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bmw122787</td>\n",
       "      <td>https://bit.ly/3qrZahY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stepan_ryabenko</td>\n",
       "      <td>https://teleg.run/stepan_ryabenko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>supersonicadu</td>\n",
       "      <td>http://www.youtube.com/user/TheSonic2124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lauren_viglietta</td>\n",
       "      <td>https://vs.co/7f9fa25d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19160</th>\n",
       "      <td>joshlozano_</td>\n",
       "      <td>http://wer1hoops.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19161</th>\n",
       "      <td>qurmaca</td>\n",
       "      <td>https://t.me/qurmaca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19162</th>\n",
       "      <td>crossedcourt</td>\n",
       "      <td>http://crossedcourt.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19163</th>\n",
       "      <td>lessy.morgan63</td>\n",
       "      <td>https://elastic-joliot-e02363.netlify.app/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19164</th>\n",
       "      <td>ktone5</td>\n",
       "      <td>https://youtu.be/dETztrPw2jk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19165 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               username                                external_url\n",
       "0              mr.145th      https://onlyfans.com/letmeholdthatsoul\n",
       "1             bmw122787                      https://bit.ly/3qrZahY\n",
       "2       stepan_ryabenko           https://teleg.run/stepan_ryabenko\n",
       "3         supersonicadu    http://www.youtube.com/user/TheSonic2124\n",
       "4      lauren_viglietta                      https://vs.co/7f9fa25d\n",
       "...                 ...                                         ...\n",
       "19160       joshlozano_                       http://wer1hoops.com/\n",
       "19161           qurmaca                        https://t.me/qurmaca\n",
       "19162      crossedcourt                    http://crossedcourt.com/\n",
       "19163    lessy.morgan63  https://elastic-joliot-e02363.netlify.app/\n",
       "19164            ktone5                https://youtu.be/dETztrPw2jk\n",
       "\n",
       "[19165 rows x 2 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query users having a link and remove those with NA\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    DISTINCT \n",
    "    username\n",
    "    , external_url \n",
    "FROM users \n",
    "WHERE external_url IS NOT NULL\"\"\"\n",
    "\n",
    "df_user_urls = pd.read_sql_query(query, con)\n",
    "df_user_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [02:28<00:00,  4.12s/it]\n"
     ]
    }
   ],
   "source": [
    "browser = Browser(\"chrome\")\n",
    "for username, external_url in tqdm(df_user_urls.values):\n",
    "    try:\n",
    "        browser.visit(external_url)\n",
    "        browser.driver.save_screenshot(f\"data/url_screenshot/{username}_external_url_screenshot.png\")\n",
    "    except:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLayground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/marclamy/Desktop/code/igbot_final/data/2_comments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6094/6094 [00:06<00:00, 938.55it/s] \n"
     ]
    }
   ],
   "source": [
    "# Make up all_comments.csv\n",
    "all_files = mu.list_files(path)\n",
    "df_all_comments_csv = pd.concat([pd.read_csv(file) for file in tqdm(all_files)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['page', 'post_id', 'legend', 'post_posted_time', 'username',\n",
       "       'full_comment_data', 'comment', 'comment_posted_time', 'comments_likes',\n",
       "       'replies', 'time_now', 'period', 'time_since_posted'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_comments_csv = df_all_comments_csv.rename({\"postid\": \"post_id\", \n",
    "                                                  \"data_collected_time\": \"time_now\"\n",
    "                                                  , \"comment_comments_count\": \"replies\"}, axis=1).drop(\"post_likes\", axis=1)\n",
    "df_all_comments_csv[\"period\"] = \"winter\"\n",
    "df_all_comments_csv[\"time_since_posted\"] = pd.to_datetime(df_all_comments_csv[\"comment_posted_time\"]) - pd.to_datetime(df_all_comments_csv[\"post_posted_time\"])\n",
    "\n",
    "df_all_comments_csv.dropna().head().columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('comment', 'comment'),\n",
       " ('comment_posted_time', 'comment_posted_time'),\n",
       " ('comments_likes', 'comments_likes'),\n",
       " ('full_comment_data', 'full_comment_data'),\n",
       " ('legend', 'legend'),\n",
       " ('page', 'page'),\n",
       " ('period', 'period'),\n",
       " ('post_id', 'post_id'),\n",
       " ('post_posted_time', 'post_posted_time'),\n",
       " ('replies', 'replies'),\n",
       " ('time_now', 'time_now'),\n",
       " ('time_since_posted', 'time_since_posted'),\n",
       " ('username', 'username')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(sorted(df_comments.columns), sorted(df_all_comments_csv.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1746"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments.to_sql(\"comments\", con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql 'select * from comments': no such table: comments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/main_env/lib/python3.8/site-packages/pandas/io/sql.py:2020\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2019\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2020\u001b[0m     cur\u001b[39m.\u001b[39;49mexecute(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2021\u001b[0m     \u001b[39mreturn\u001b[39;00m cur\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such table: comments",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/marclamy/Desktop/code/instagram_bot_detection/1_data_collection.ipynb Cell 47\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/marclamy/Desktop/code/instagram_bot_detection/1_data_collection.ipynb#Y150sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df_comments \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_sql_query(\u001b[39m\"\u001b[39;49m\u001b[39mselect * from comments\u001b[39;49m\u001b[39m\"\u001b[39;49m, con)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/marclamy/Desktop/code/instagram_bot_detection/1_data_collection.ipynb#Y150sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m cols \u001b[39m=\u001b[39m df_comments\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/marclamy/Desktop/code/instagram_bot_detection/1_data_collection.ipynb#Y150sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df_all_comments_csv[cols]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/main_env/lib/python3.8/site-packages/pandas/io/sql.py:399\u001b[0m, in \u001b[0;36mread_sql_query\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[39mRead SQL query into a DataFrame.\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[39mparameter will be converted to UTC.\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    398\u001b[0m pandas_sql \u001b[39m=\u001b[39m pandasSQL_builder(con)\n\u001b[0;32m--> 399\u001b[0m \u001b[39mreturn\u001b[39;00m pandas_sql\u001b[39m.\u001b[39;49mread_query(\n\u001b[1;32m    400\u001b[0m     sql,\n\u001b[1;32m    401\u001b[0m     index_col\u001b[39m=\u001b[39;49mindex_col,\n\u001b[1;32m    402\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    403\u001b[0m     coerce_float\u001b[39m=\u001b[39;49mcoerce_float,\n\u001b[1;32m    404\u001b[0m     parse_dates\u001b[39m=\u001b[39;49mparse_dates,\n\u001b[1;32m    405\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[1;32m    406\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    407\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/main_env/lib/python3.8/site-packages/pandas/io/sql.py:2080\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[0;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize, dtype)\u001b[0m\n\u001b[1;32m   2068\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_query\u001b[39m(\n\u001b[1;32m   2069\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   2070\u001b[0m     sql,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2076\u001b[0m     dtype: DtypeArg \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2077\u001b[0m ):\n\u001b[1;32m   2079\u001b[0m     args \u001b[39m=\u001b[39m _convert_params(sql, params)\n\u001b[0;32m-> 2080\u001b[0m     cursor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m   2081\u001b[0m     columns \u001b[39m=\u001b[39m [col_desc[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m col_desc \u001b[39min\u001b[39;00m cursor\u001b[39m.\u001b[39mdescription]\n\u001b[1;32m   2083\u001b[0m     \u001b[39mif\u001b[39;00m chunksize \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/main_env/lib/python3.8/site-packages/pandas/io/sql.py:2032\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2029\u001b[0m     \u001b[39mraise\u001b[39;00m ex \u001b[39mfrom\u001b[39;00m \u001b[39minner_exc\u001b[39;00m\n\u001b[1;32m   2031\u001b[0m ex \u001b[39m=\u001b[39m DatabaseError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExecution failed on sql \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00margs[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mexc\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2032\u001b[0m \u001b[39mraise\u001b[39;00m ex \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql 'select * from comments': no such table: comments"
     ]
    }
   ],
   "source": [
    "df_comments = pd.read_sql_query(\"select * from comments\", con)\n",
    "cols = df_comments.columns\n",
    "df_all_comments_csv[cols]\n",
    "\n",
    "df_concat = pd.concat([df_comments, df_all_comments_csv], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_concat[\"comment_posted_time\"] = df_concat[\"comment_posted_time\"]\n",
    "df_concat[\"comment_posted_time\"] = pd.to_datetime(df_concat[\"comment_posted_time\"]).astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 136211 entries, 0 to 11\n",
      "Data columns (total 13 columns):\n",
      " #   Column               Non-Null Count   Dtype \n",
      "---  ------               --------------   ----- \n",
      " 0   post_id              136211 non-null  object\n",
      " 1   page                 136211 non-null  object\n",
      " 2   legend               136211 non-null  object\n",
      " 3   post_posted_time     136211 non-null  object\n",
      " 4   username             136211 non-null  object\n",
      " 5   full_comment_data    136211 non-null  object\n",
      " 6   comment              136211 non-null  object\n",
      " 7   comment_posted_time  136211 non-null  object\n",
      " 8   time_since_posted    136211 non-null  object\n",
      " 9   comments_likes       136211 non-null  object\n",
      " 10  replies              136211 non-null  object\n",
      " 11  time_now             136211 non-null  object\n",
      " 12  period               136211 non-null  object\n",
      "dtypes: object(13)\n",
      "memory usage: 14.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_concat.to_sql(\"comments\", con, if_exists=\"replace\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('main_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06d12861cad82bdcdde1b56bd9eda52e91f7df29dabbeda8f3d9112222750302"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
